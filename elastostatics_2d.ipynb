{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import stax, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pinns \n",
    "import datetime\n",
    "import jax.scipy.optimize\n",
    "import jax.flatten_util\n",
    "import scipy\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "rnd_key = jax.random.PRNGKey(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geometry(key, scale = 1):\n",
    "    R = 2\n",
    "    r = 1\n",
    "    \n",
    "    knots = np.array([[[r,0],[R,0]],[[r,r],[R,R]],[[0,r],[0,R]]])\n",
    "    weights = np.ones(knots.shape[:2])\n",
    "    weights[1,:] = 1/np.sqrt(2)\n",
    "\n",
    "    basis2 = pinns.bspline.BSplineBasisJAX(np.linspace(-1,1,2),1)\n",
    "    basis1 = pinns.bspline.BSplineBasisJAX(np.array([-1,1]),2)\n",
    "\n",
    "    geom = pinns.geometry.PatchNURBSParam([basis1, basis2], knots, weights, 0, 2, key)\n",
    "    return  geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = create_geometry(rnd_key)\n",
    "\n",
    "pts,_ = geom.importance_sampling(10000)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(pts[:,0], pts[:,1], s = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interface_function2d(nd, endpositive, endzero, nn):\n",
    "\n",
    "    faux = lambda x: ((x-endzero)**1/(endpositive-endzero)**1)\n",
    "    if nd == 0:\n",
    "        fret = lambda ws, x: (nn(ws, x[...,1][...,None]).flatten()*faux(x[...,0]))[...,None]\n",
    "    else:\n",
    "        fret = lambda ws, x: (nn(ws, x[...,0][...,None]).flatten()*faux(x[...,1]))[...,None]\n",
    "    return fret\n",
    "\n",
    "def jump_function2d(nd, pos_y, nn):\n",
    "\n",
    "    faux = lambda x: jnp.exp(-4.0*jnp.abs(x-pos_y))\n",
    "    if nd == 1:\n",
    "        fret = lambda ws, x: (nn(ws, x[...,1][...,None]).flatten()*faux(x[...,0]))[...,None]\n",
    "    else:\n",
    "        fret = lambda ws, x: (nn(ws, x[...,0][...,None]).flatten()*faux(x[...,1]))[...,None]\n",
    "    return fret\n",
    "\n",
    "# def ExpHat(hidden_size, out_dim, W_init=glorot_normal(), b_init=normal()):\n",
    "#   \"\"\"Layer constructor function for a dense (fully-connected) layer.\"\"\"\n",
    "#   def init_fun(rng, input_shape):\n",
    "#     output_shape = input_shape[:-1] + (out_dim,)\n",
    "#     k1, k2 = random.split(rng)\n",
    "#     W, b = W_init(k1, (input_shape[-1], out_dim)), b_init(k2, (out_dim,))\n",
    "#     return output_shape, (W, b)\n",
    "#   def apply_fun(params, inputs, **kwargs):\n",
    "#     b, b = params\n",
    "#     return jnp.dot(inputs, W) + b\n",
    "#   return init_fun, apply_fun\n",
    "def ExpHat(x, scale = 0.1):\n",
    "    return jnp.exp(-jnp.abs(x)/scale)\n",
    "\n",
    "class Model(pinns.PINN):\n",
    "    def __init__(self, rand_key):\n",
    "        super().__init__()\n",
    "        self.key = rand_key\n",
    "\n",
    "        nl = 10\n",
    "        acti =  stax.elementwise(lambda x: jax.nn.leaky_relu(x)**2)\n",
    "\n",
    "        \n",
    "        block_first = stax.serial(stax.FanOut(2),stax.parallel(stax.serial(stax.Dense(nl), acti, stax.Dense(nl), acti),stax.Dense(nl)),stax.FanInSum)\n",
    "        block = stax.serial(stax.FanOut(2),stax.parallel(stax.serial(stax.Dense(nl), acti, stax.Dense(nl), acti),stax.Dense(nl)),stax.FanInSum)\n",
    "        \n",
    "        self.add_neural_network('u1',stax.serial(block_first,block,block, block, stax.Dense(2)),(-1,2)) # iron\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.points = self.get_points_MC(100000, self.key)\n",
    "        \n",
    "        E = 0.02e4\n",
    "        nu = 0.1\n",
    "        self.E = E\n",
    "        self.nu = nu\n",
    "        \n",
    "        self.lamda = E*nu/(1+nu)/(1-2*nu)\n",
    "        self.mu = E/2/(1+nu)\n",
    "        \n",
    "        rho = 0.2\n",
    "        g = 9.81\n",
    "        self.rho = rho\n",
    "        \n",
    "        self.f = np.array([0.0,-g*rho])\n",
    "        \n",
    "        self.c = self.lamda * np.einsum('ij,kl->ijkl', np.eye(2), np.eye(2)) + self.mu * (np.einsum('ik,jl->ijkl',np.eye(2),np.eye(2)) + np.einsum('il,jk->ijkl',np.eye(2),np.eye(2)))\n",
    "        \n",
    "    def get_points_MC(self, N, key):        \n",
    "\n",
    "        points = {}\n",
    "\n",
    "\n",
    "        ys = jax.random.uniform(key ,(N,2))*2-1\n",
    "        Weights = jnp.ones((N,))*4/ys.shape[0]\n",
    "        # ys = np.array(jax.random.uniform(self.key, (N,2)))*2-1\n",
    "        # Weights = jnp.ones((N,))*4/ys.shape[0]\n",
    "\n",
    "\n",
    "        points['ys1'] = ys\n",
    "        points['ws1'] = Weights\n",
    "        points['omega1'], points['G1'], points['K1'] = geom.GetMetricTensors(ys)\n",
    "       \n",
    "\n",
    "\n",
    "        return points\n",
    "\n",
    "\n",
    "    def solution1(self, ws, y):\n",
    "        # iron\n",
    "        alpha = 2\n",
    "        u = self.neural_networks['u1'](ws['u1'],y)\n",
    "        v = ((1-y[...,0]))[...,None]\n",
    "        \n",
    "        return u*jnp.concatenate((v,v),-1)\n",
    "\n",
    "\n",
    "\n",
    "    def loss_pde(self, ws, points):\n",
    "        jacs = pinns.operators.jacobian(lambda x : self.solution1(ws,x))(points['ys1'])\n",
    "        J = geom.GetJacobian(points['ys1'])\n",
    "        \n",
    "        jacs_x = jnp.einsum('mij,mjk->mik',jacs,points['G1'])\n",
    "        divs_x = (jacs_x[...,0,0]+jacs_x[...,1,1])[...,None]\n",
    "        \n",
    "        strain = 0.5*(jacs_x+jnp.transpose(jacs_x,[0,2,1]))\n",
    "        \n",
    "        stress = self.lamda * jnp.einsum('ij,jkl->ikl',divs_x, jnp.eye(2)[None,...]) +2*self.mu*strain\n",
    "        # D = jnp.eye(4)\n",
    "        \n",
    "        # BTDB = jnp.einsum('ji,jk,kl->il',B,D,B)\n",
    "        \n",
    "        # a = jnp.dot(jnp.einsum('mi,mij,mj->m',gradsx,BTDB,gradsx), points['ws1']*points['omega1']) \n",
    "\n",
    "        a = 0.5*jnp.dot(jnp.einsum('mij,mij->m',stress, strain), points['ws1']*points['omega1']) \n",
    "        # a = jnp.dot(jnp.einsum('mij,ijkl,mkl->m',strain,self.c,strain), points['ws1']*points['omega1']) \n",
    "        # rhs = jnp.dot(jnp.einsum('j,mkj,mk->m',self.f,J,self.solution1(ws,points['ys1'])), points['omega1']*points['ws1'])\n",
    "        rhs = jnp.dot(jnp.einsum('k,mk->m',self.f,self.solution1(ws,points['ys1'])), points['omega1']*points['ws1'])\n",
    "        # rhs = jnp.dot(jnp.einsum('i,mij,mj->m',self.f, points['K1'], self.solution1(ws,points['ys1'])), points['ws1'])\n",
    "        return a-rhs\n",
    "\n",
    "    def loss(self, ws, pts):\n",
    "        lpde = self.loss_pde(ws, pts)\n",
    "        return lpde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnd_key = jax.random.PRNGKey(1235)\n",
    "model = Model(rnd_key)\n",
    "w0 = model.init_unravel()\n",
    "weights = model.weights \n",
    "\n",
    "dev = jax.devices('gpu')[1]\n",
    "\n",
    "# loss_compiled = jax.jit(model.loss_handle, device = jax.devices()[0])\n",
    "# lossgrad_compiled = jax.jit(model.lossgrad_handle, device = jax.devices()[0])\n",
    "# \n",
    "# def loss_grad(w):\n",
    "#     l, gr = lossgrad_compiled(jnp.array(w))\n",
    "#     return np.array( l.to_py() ), np.array( gr.to_py() )\n",
    "\n",
    "opt_type = 'LBFGS'\n",
    "\n",
    "if opt_type == 'ADAM':\n",
    "\n",
    "    batch_size = 5000\n",
    "\n",
    "    get_compiled = jax.jit(lambda key: model.get_points_MC(batch_size, key), device = dev)\n",
    "    %time pts = get_compiled(jax.random.PRNGKey(1235))\n",
    "    %time pts = get_compiled(jax.random.PRNGKey(1111))\n",
    "\n",
    "    lr_opti = optimizers.piecewise_constant([2000,4000,6000,8000,12000], [0.01, 0.005, 0.001, 0.0005,0.0001,0.00001])\n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr_opti)\n",
    "\n",
    "    opt_state = opt_init(weights)\n",
    "\n",
    "    # get initial parameters\n",
    "    params = get_params(opt_state)\n",
    "\n",
    "    loss_grad = jax.jit(lambda ws, pts: (model.loss(ws, pts), jax.grad(model.loss)(ws, pts)), device = dev)\n",
    "\n",
    "    def step(params, opt_state, key):\n",
    "        # points = model.get_points_MC(5000)\n",
    "        points = model.get_points_MC(batch_size, key)\n",
    "        loss, grads = loss_grad(params, points)\n",
    "        opt_state = opt_update(0, grads, opt_state)\n",
    "\n",
    "        params = get_params(opt_state)\n",
    "        \n",
    "        return params, opt_state, loss\n",
    "\n",
    "    step_compiled = jax.jit(step, device = dev)\n",
    "    step_compiled(params, opt_state, rnd_key)\n",
    "\n",
    "    n_epochs = 14000\n",
    "\n",
    "    hist = []\n",
    "\n",
    "    tme = datetime.datetime.now()\n",
    "    for k in range(n_epochs):    \n",
    "        params, opt_state, loss = step_compiled(params, opt_state, jax.random.PRNGKey(np.random.randint(32131233123)))\n",
    "        \n",
    "        hist.append(loss)\n",
    "        \n",
    "        print('Epoch %d/%d - loss value %e'%(k+1, n_epochs, loss))\n",
    "    # update params\n",
    "    model.weights = params\n",
    "    weights = params\n",
    "    tme = datetime.datetime.now() - tme\n",
    "    print('Elapsed time ', tme)\n",
    "elif opt_type == 'LBFGS':\n",
    "\n",
    "    points = model.get_points_MC(10000, rnd_key)\n",
    "\n",
    "    lossgrad_compiled = jax.jit(model.lossgrad_handle, device = dev)\n",
    "    \n",
    "    def loss_grad(w):\n",
    "        l, gr = lossgrad_compiled(jnp.array(w), points)\n",
    "        return np.array( l.to_py() ), np.array( gr.to_py() )\n",
    "\n",
    "    tme = datetime.datetime.now()\n",
    "    #results = jax.scipy.optimize.minimize(loss_grad, x0 = weights_vector, method = 'bfgs', options = {'maxiter': 10})\n",
    "    # result = scipy.optimize.minimize(loss_grad, x0 = w0.to_py(), method = 'BFGS', jac = True, tol = 1e-8, options = {'disp' : True, 'maxiter' : 2000}, callback = None)\n",
    "    result = scipy.optimize.minimize(loss_grad, x0 = w0.to_py(), method = 'L-BFGS-B', jac = True, tol = 1e-19, options = {'disp' : True, 'maxiter' : 3000, 'iprint': 1})\n",
    "    tme = datetime.datetime.now() - tme\n",
    "\n",
    "    weights = model.weights_unravel(jnp.array(result.x))\n",
    "    model.weights = weights\n",
    "    print()\n",
    "    print('Elapsed time', tme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "\n",
    "# plt.loglog(np.arange(n_epochs)+1,np.array(hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yspace = np.linspace(-1,1,128)\n",
    "ysteps = np.linspace(-1,1,8)\n",
    "\n",
    "fh = jax.jit(lambda ys: (geom(ys), geom.GetJacobian(ys), model.solution1(weights, ys)) )\n",
    "\n",
    "plt.figure()\n",
    "for y in ysteps:\n",
    "    ys  = jnp.array(np.concatenate((yspace[...,None],yspace[...,None]*0+y),-1))\n",
    "    Xs, Js, Us = fh(ys)\n",
    "\n",
    "    # Us = jnp.ones([128,2])*jnp.array([1,0])\n",
    "    # Us = jnp.einsum('mji,mi->mj',Js,Us)\n",
    "    plt.plot(Xs[:,0],Xs[:,1],'grey')\n",
    "    plt.plot(Xs[:,0]+Us[:,0],Xs[:,1]+Us[:,1],'blue')\n",
    "    \n",
    "    ys = jnp.array(np.concatenate((yspace[...,None]*0+y,yspace[...,None]),-1))\n",
    "    Xs, Js, Us = fh(ys)\n",
    "\n",
    "    # Us = jnp.ones([128,2])*jnp.array([1,0])\n",
    "    # Us = jnp.einsum('mji,mi->mj',Js,Us)\n",
    "    plt.plot(Xs[:,0],Xs[:,1],'grey')\n",
    "    plt.plot(Xs[:,0]+Us[:,0],Xs[:,1]+Us[:,1],'blue')\n",
    "\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.savefig('./elasto_2d.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from elastostatics_2d_fem import FEM\n",
    "\n",
    "    fem = FEM(model.E, model.nu, model.rho)\n",
    "    fem.solve(0.025)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    for y in ysteps:\n",
    "        ys  = jnp.array(np.concatenate((yspace[...,None],yspace[...,None]*0+y),-1))\n",
    "        Xs, Js, Us = fh(ys)\n",
    "        #Us = jnp.einsum('mji,mi->mj',Js,Us)\n",
    "        Us_fem = fem(np.array(Xs))\n",
    "        plt.plot(Xs[:,0]+Us_fem[:,0],Xs[:,1]+Us_fem[:,1],'red')\n",
    "        plt.plot(Xs[:,0]+Us[:,0],Xs[:,1]+Us[:,1],'blue')\n",
    "        \n",
    "        ys = jnp.array(np.concatenate((yspace[...,None]*0+y,yspace[...,None]),-1))\n",
    "        Xs, Js, Us = fh(ys)\n",
    "        #Us = jnp.einsum('mji,mi->mj',Js,Us)\n",
    "        Us_fem = fem(np.array(Xs))\n",
    "        plt.plot(Xs[:,0]+Us_fem[:,0],Xs[:,1]+Us_fem[:,1],'red')\n",
    "        plt.plot(Xs[:,0]+Us[:,0],Xs[:,1]+Us[:,1],'blue')\n",
    "        \n",
    "        \n",
    "except:\n",
    "    print(\"FENICS not available.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('fenics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b367a7c9ddec1ad018e6784e48bc91e6ee34e9f38962cdfdd9842dda11c9ecd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
